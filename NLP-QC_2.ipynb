{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMTqrGzgeTMocsvJW5k0h5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunil-karki/NLP-Question_Classification/blob/master/NLP-QC_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwvoJLjaaFDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## https://www.kaggle.com/c/quora-insincere-questions-classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en6DVTgOac22",
        "colab_type": "text"
      },
      "source": [
        "* Based on SRK's kernel: https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings\n",
        "* Vladimir Demidov's 2DCNN textClassifier: https://www.kaggle.com/yekenot/2dcnn-textclassifier\n",
        "* Attention layer from Khoi Ngyuen: https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
        "* LSTM model from Strideradu: https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
        "* https://www.kaggle.com/danofer/different-embeddings-with-attention-fork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6F7fbtBJUjc",
        "colab_type": "code",
        "outputId": "8aaf36e5-e0a1-4760-bac6-ac064e79b064",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f25b2d79-a5b2-4388-b74e-2f09d809e5c1\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f25b2d79-a5b2-4388-b74e-2f09d809e5c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"sunilkarki520\",\"key\":\"8368d7fdc07a580b54aeb9a6df65cda4\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1EwcdDFWyiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmxCY2M6WEeu",
        "colab_type": "code",
        "outputId": "9253620e-b3c3-45e4-a8d9-09b2513e2fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!kaggle competitions download -c quora-insincere-questions-classification"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/4.09M [00:00<?, ?B/s]\n",
            "\r100% 4.09M/4.09M [00:00<00:00, 67.7MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 82% 13.0M/15.8M [00:00<00:00, 17.0MB/s]\n",
            "100% 15.8M/15.8M [00:00<00:00, 20.3MB/s]\n",
            "Downloading embeddings.zip to /content\n",
            "100% 5.95G/5.96G [06:41<00:00, 20.1MB/s]\n",
            "100% 5.96G/5.96G [06:42<00:00, 15.9MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "100% 54.9M/54.9M [00:04<00:00, 14.5MB/s]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yupHrgncW5rd",
        "colab_type": "code",
        "outputId": "310f897a-84de-4941-a46c-c0544adf5595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings.zip\t\t\tparagram_300_sl999\t   train.csv.zip\n",
            "glove.840B.300d\t\t\tsample_data\t\t   wiki-news-300d-1M\n",
            "GoogleNews-vectors-negative300\tsample_submission.csv.zip\n",
            "kaggle.json\t\t\ttest.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z-Uv2o5XcTn",
        "colab_type": "code",
        "outputId": "29a0dd10-ca2a-483b-9a6b-dd4ab4a62c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!unzip embeddings.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  embeddings.zip\n",
            "   creating: GoogleNews-vectors-negative300/\n",
            "   creating: glove.840B.300d/\n",
            "   creating: paragram_300_sl999/\n",
            "   creating: wiki-news-300d-1M/\n",
            "  inflating: glove.840B.300d/glove.840B.300d.txt  \n",
            "  inflating: GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin  \n",
            "  inflating: wiki-news-300d-1M/wiki-news-300d-1M.vec  \n",
            "  inflating: paragram_300_sl999/README.txt  \n",
            "  inflating: paragram_300_sl999/paragram_300_sl999.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOHXDmLlXlq0",
        "colab_type": "code",
        "outputId": "5bc3c795-4f89-47ad-a5a6-a207cd0e8989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings.zip\t\t\tparagram_300_sl999\t   train.csv.zip\n",
            "glove.840B.300d\t\t\tsample_data\t\t   wiki-news-300d-1M\n",
            "GoogleNews-vectors-negative300\tsample_submission.csv.zip\n",
            "kaggle.json\t\t\ttest.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_AvYRaAYpUR",
        "colab_type": "code",
        "outputId": "91108cb6-61b8-4d8f-8da2-41d18cdf825e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls wiki-news-300d-1M"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wiki-news-300d-1M.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wvesheAdltp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edHfF8bta_Cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8xhRTDqbTE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## some config values \n",
        "embed_size = 300 # how big is each word vector\n",
        "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 70 # max number of words in a question to use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G149cSt1bUSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "9da9d06f-69fe-4675-b613-a42128ae851c"
      },
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-8Ho5O_81e_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.optimizer.set_jit(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5mrsRcw9PRg",
        "colab_type": "code",
        "outputId": "7609f25f-2938-4c07-98fd-590c22bb9e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!df -h"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   50G   53G  49% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "tmpfs            13G     0   13G   0% /sys/fs/cgroup\n",
            "tmpfs            13G  8.0K   13G   1% /var/colab\n",
            "/dev/sda1       114G   51G   64G  44% /etc/hosts\n",
            "shm              13G  4.0K   13G   1% /dev/shm\n",
            "tmpfs            13G     0   13G   0% /proc/acpi\n",
            "tmpfs            13G     0   13G   0% /proc/scsi\n",
            "tmpfs            13G     0   13G   0% /sys/firmware\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poMibfZa9fmL",
        "colab_type": "code",
        "outputId": "7ac7f244-26bd-48cd-a34e-5b4dfc3ebc80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TJrwlV0bZNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_prec():\n",
        "    train_df = pd.read_csv('train.csv.zip', compression='zip') \n",
        "    test_df = pd.read_csv('test.csv.zip', compression='zip') \n",
        "    print(\"Train shape : \",train_df.shape)\n",
        "    print(\"Test shape : \",test_df.shape)\n",
        "    \n",
        "    ## split to train and val\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.001, random_state=2018)\n",
        "\n",
        "\n",
        "    ## fill up the missing values\n",
        "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
        "    val_X = val_df[\"question_text\"].fillna(\"_##_\").values\n",
        "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
        "\n",
        "    ## Tokenize the sentences\n",
        "    tokenizer = Tokenizer(num_words=max_features)\n",
        "    tokenizer.fit_on_texts(list(train_X))\n",
        "    train_X = tokenizer.texts_to_sequences(train_X)\n",
        "    val_X = tokenizer.texts_to_sequences(val_X)\n",
        "    test_X = tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "    ## Pad the sentences \n",
        "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "    val_X = pad_sequences(val_X, maxlen=maxlen)\n",
        "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
        "\n",
        "    ## Get the target values\n",
        "    train_y = train_df['target'].values\n",
        "    val_y = val_df['target'].values  \n",
        "    \n",
        "    #shuffling the data\n",
        "    np.random.seed(2018)\n",
        "    trn_idx = np.random.permutation(len(train_X))\n",
        "    val_idx = np.random.permutation(len(val_X))\n",
        "\n",
        "    train_X = train_X[trn_idx]\n",
        "    val_X = val_X[val_idx]\n",
        "    train_y = train_y[trn_idx]\n",
        "    val_y = val_y[val_idx]    \n",
        "    \n",
        "    return train_X, val_X, test_X, train_y, val_y, tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irPF54DqLQG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "a9f9a876-d1cd-4779-f8c0-28e7d654d55d"
      },
      "source": [
        "#Just testing\n",
        "train_df = pd.read_csv('train.csv.zip', compression='zip') \n",
        "test_df = pd.read_csv('test.csv.zip', compression='zip') \n",
        "print(\"Train shape : \",train_df.shape)\n",
        "print(\"Test shape : \",test_df.shape)\n",
        "train_df[train_df[\"target\"]==1]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (1306122, 3)\n",
            "Test shape :  (375806, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0000e91571b60c2fb487</td>\n",
              "      <td>Has the United States become the largest dicta...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>00013ceca3f624b09f42</td>\n",
              "      <td>Which babies are more sweeter to their parents...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>0004a7fcb2bf73076489</td>\n",
              "      <td>If blacks support school choice and mandatory ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>00052793eaa287aff1e1</td>\n",
              "      <td>I am gay boy and I love my cousin (boy). He is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>000537213b01fd77b58a</td>\n",
              "      <td>Which races have the smallest penis?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306093</th>\n",
              "      <td>fffeba722d9b371bd1b9</td>\n",
              "      <td>How is it to have intimate relation with your ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306094</th>\n",
              "      <td>fffee269360dd0d3947a</td>\n",
              "      <td>Why is it when singers have lyrics about voice...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306099</th>\n",
              "      <td>ffff0e4ea1bb6e16feec</td>\n",
              "      <td>Do pakis smell of curry and shit?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306103</th>\n",
              "      <td>ffff3f0a2449ffe4b9ff</td>\n",
              "      <td>Isn't Trump right after all? Why should the US...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306112</th>\n",
              "      <td>ffffa5b0fa76431c063f</td>\n",
              "      <td>Are you ashamed of being an Indian?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80810 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                          qid  ... target\n",
              "22       0000e91571b60c2fb487  ...      1\n",
              "30       00013ceca3f624b09f42  ...      1\n",
              "110      0004a7fcb2bf73076489  ...      1\n",
              "114      00052793eaa287aff1e1  ...      1\n",
              "115      000537213b01fd77b58a  ...      1\n",
              "...                       ...  ...    ...\n",
              "1306093  fffeba722d9b371bd1b9  ...      1\n",
              "1306094  fffee269360dd0d3947a  ...      1\n",
              "1306099  ffff0e4ea1bb6e16feec  ...      1\n",
              "1306103  ffff3f0a2449ffe4b9ff  ...      1\n",
              "1306112  ffffa5b0fa76431c063f  ...      1\n",
              "\n",
              "[80810 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HvqcHpbO3h0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "08212696-78b4-4d99-a33b-e7593951f716"
      },
      "source": [
        "#Just testing\n",
        "# EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
        "EMBEDDING_FILE = 'glove.840B.300d/glove.840B.300d.txt'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "#embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "for o in open(EMBEDDING_FILE):\n",
        "    #print(o)    #Glass -0.41236 0.35532 -0.076236 0.43268 0.14769 -1.1381 -0.30427 0.37039 -0.3804 0.029443 -0.081454 0.15791 -0.32558 -0.35445 0.11918 -0.13342 -0.087387 1.2125 -0.048223  .....\n",
        "    print(o.split(\" \"))\n",
        "'''\n",
        "#print(embeddings_index)\n",
        "with open(\"glove.840B.300d/glove.840B.300d.txt\") as myfile:\n",
        "    head = [next(myfile) for x in range(10)]\n",
        "    f = open(\"demofile3.txt\", \"w\")\n",
        "    f.write(str(head))\n",
        "    f.close()\n",
        "print(head)\n",
        "'''\n",
        "\n",
        "#embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open('demofile3.txt'))\n",
        "#print(embeddings_index)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['field-group', '0.32129', '-1.6378', '0.75596', '0.26813', '0.47293', '0.26158', '0.85073', '0.29399', '-0.26904', '-2.2186', '0.31221', '-0.50201', '0.11557', '0.24599', '1.3028', '0.36848', '0.84239', '-1.527', '0.37717', '-0.40599', '0.012609', '0.46039', '0.12695', '0.002067', '0.0069745', '-1.0914', '0.29317', '-0.30063', '0.0089122', '-0.11496', '0.40786', '-0.62797', '0.169', '-0.25301', '-0.51534', '0.65677', '-0.99634', '-0.34599', '0.017932', '0.58895', '0.29872', '0.011406', '-0.46086', '-0.53757', '0.2863', '-0.62271', '0.1287', '-1.1615', '-0.6878', '-0.38682', '-0.54675', '0.597', '-0.55561', '0.25024', '1.0137', '1.4105', '-0.25873', '-0.42906', '-0.30526', '-0.22734', '-0.2962', '0.0037484', '0.56168', '-1.1897', '-0.76716', '0.059877', '0.3831', '0.59069', '0.11401', '0.41028', '-0.20433', '-0.24174', '0.28124', '0.86143', '-0.57543', '-1.369', '-1.5294', '0.95703', '1.0324', '-0.44549', '0.3984', '0.092365', '-1.173', '-0.032192', '-0.68014', '0.60831', '0.27091', '0.4973', '-1.4823', '0.24152', '-0.5793', '-0.98738', '0.44784', '-0.37943', '-1.7265', '0.89062', '0.34291', '0.30475', '0.12068', '0.22688', '0.23377', '-0.5168', '0.016103', '0.010667', '0.524', '0.02407', '-0.1006', '-0.28487', '-0.6828', '0.87968', '-1.3747', '1.1508', '-0.3463', '0.23373', '0.85396', '-0.19368', '-0.43133', '0.34741', '0.3733', '-0.74125', '-0.29424', '0.47343', '0.033889', '0.30558', '0.30758', '0.64758', '-0.14085', '0.49471', '-0.073976', '0.4823', '0.00060837', '0.24565', '0.41062', '-0.17748', '-0.47863', '0.62015', '-0.11738', '0.27769', '-0.94306', '0.062649', '0.24707', '-0.19218', '-0.66366', '0.27595', '0.76457', '0.52513', '0.27557', '0.22029', '0.018922', '-0.27005', '-0.13193', '0.072728', '-0.35305', '0.62783', '0.021852', '-0.23966', '-1.2064', '-0.48168', '-0.48974', '0.03551', '-0.17829', '0.71997', '0.39223', '0.20216', '0.83855', '0.51976', '-0.46003', '0.13679', '0.55635', '-0.40344', '0.44923', '0.9749', '-0.50361', '-0.17999', '-0.042185', '-0.70282', '-1.4099', '-0.29828', '-0.82485', '-0.8998', '-0.076104', '0.28228', '-0.033434', '0.47737', '-0.23042', '-0.16322', '0.084106', '0.25893', '0.20352', '0.035414', '1.2138', '0.41616', '-0.26835', '-0.33488', '-0.094038', '0.52437', '0.26174', '0.35835', '-0.77192', '-0.4465', '-0.48862', '0.32006', '0.076231', '-0.0074974', '-0.72496', '0.2061', '-0.3629', '0.36627', '0.53831', '0.02439', '-0.023917', '-0.13354', '-0.24872', '0.18626', '0.35747', '-0.27656', '0.042172', '0.47327', '0.059069', '-0.23075', '0.50054', '-0.16866', '0.18549', '0.3536', '0.84043', '-0.5011', '0.382', '-1.0947', '-0.11403', '-0.18575', '0.0068315', '0.28151', '0.91882', '-1.18', '0.10806', '0.50037', '-0.33117', '0.3209', '-0.10392', '-0.23952', '-0.40787', '0.75501', '0.092337', '-0.43005', '-0.84078', '0.59387', '0.93811', '0.14778', '-0.3834', '-0.96825', '0.34963', '0.50347', '-0.70492', '0.47509', '0.2627', '-0.53628', '0.32531', '-0.35787', '-0.37315', '0.49656', '-0.096573', '-0.5553', '-0.12555', '-1.1027', '-0.13048', '0.23343', '-0.68322', '-0.41472', '0.1285', '1.0221', '-0.25996', '-0.17504', '0.45521', '0.40365', '0.60096', '0.36485', '-0.60752', '-0.72941', '-0.31271', '0.26015', '-1.0295', '0.22247', '0.53699', '-0.062247', '0.85277', '-0.25858', '-1.2607', '-0.18543', '-1.0424', '0.99148', '0.95234', '-0.34289', '0.03682', '-0.18517', '0.71432', '-0.21646', '0.46557', '0.27077', '-0.038764', '-1.201\\n']"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-eb8192562799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#print(o)    #Glass -0.41236 0.35532 -0.076236 0.43268 0.14769 -1.1381 -0.30427 0.37039 -0.3804 0.029443 -0.081454 0.15791 -0.32558 -0.35445 0.11918 -0.13342 -0.087387 1.2125 -0.048223  .....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m '''\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(embeddings_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mevent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mevent_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mevent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHjfce7CQjar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "61d36bce-0750-4d59-e2ac-453ea6938af6"
      },
      "source": [
        "txt = \"Glass -0.41236 0.35532 -0.076236 0.43268 0.14769 -1.1381 -0.30427 0.37039 -0.3804 0.029443 -0.081454 0.15791 -0.32558 -0.35445 0.11918 -0.13342 -0.087387 1.2125 -0.048223\"\n",
        "\n",
        "def get_coefs(word,*arr):\n",
        "  print(word)\n",
        "  print(np.asarray(arr, dtype='float32'))\n",
        "  return word,np.asarray(arr, dtype='float32')\n",
        "\n",
        "\n",
        "#print(word)\n",
        "#print(np.asarray(arr, dtype='float32'))\n",
        "#return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "x = txt.split()\n",
        "\n",
        "print(x)\n",
        "\n",
        "get_coefs(*txt.split(\" \"))\n",
        "print()\n",
        "#embeddings_index = dict(get_coefs(*txt.split(\" \")))\n",
        "\n",
        "def get_coefs_1(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs_1(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "print(type(embeddings_index))\n",
        "print(embeddings_index)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Glass', '-0.41236', '0.35532', '-0.076236', '0.43268', '0.14769', '-1.1381', '-0.30427', '0.37039', '-0.3804', '0.029443', '-0.081454', '0.15791', '-0.32558', '-0.35445', '0.11918', '-0.13342', '-0.087387', '1.2125', '-0.048223']\n",
            "Glass\n",
            "[-0.41236   0.35532  -0.076236  0.43268   0.14769  -1.1381   -0.30427\n",
            "  0.37039  -0.3804    0.029443 -0.081454  0.15791  -0.32558  -0.35445\n",
            "  0.11918  -0.13342  -0.087387  1.2125   -0.048223]\n",
            "\n",
            "Glass\n",
            "[-0.41236   0.35532  -0.076236  0.43268   0.14769  -1.1381   -0.30427\n",
            "  0.37039  -0.3804    0.029443 -0.081454  0.15791  -0.32558  -0.35445\n",
            "  0.11918  -0.13342  -0.087387  1.2125   -0.048223]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b672cdb767bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mget_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 5; 2 is required"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o709E4RGUQn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt = \"Water -0.41236 0.35532 -0.076236 0.43268 0.14769 -1.1381 -0.30427 0.37039 -0.3804 0.029443 -0.081454 0.15791 -0.32558 -0.35445 0.11918 -0.13342 -0.087387 1.2125 -0.048223\\n\"\n",
        "\n",
        "file1 = open(\"myfile1.txt\",\"a\")\n",
        "file1.write(txt) \n",
        "file1.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKH3kkTuVs9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "132ce5a4-05ea-47f3-d1ec-7ade7eca43f5"
      },
      "source": [
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(\"myfile1.txt\"))\n",
        "print(type(embeddings_index))\n",
        "print(embeddings_index)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "{'Glass': array([-0.41236 ,  0.35532 , -0.076236,  0.43268 ,  0.14769 , -1.1381  ,\n",
            "       -0.30427 ,  0.37039 , -0.3804  ,  0.029443, -0.081454,  0.15791 ,\n",
            "       -0.32558 , -0.35445 ,  0.11918 , -0.13342 , -0.087387,  1.2125  ,\n",
            "       -0.048223], dtype=float32), 'Water': array([-0.41236 ,  0.35532 , -0.076236,  0.43268 ,  0.14769 , -1.1381  ,\n",
            "       -0.30427 ,  0.37039 , -0.3804  ,  0.029443, -0.081454,  0.15791 ,\n",
            "       -0.32558 , -0.35445 ,  0.11918 , -0.13342 , -0.087387,  1.2125  ,\n",
            "       -0.048223], dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHP454WDWTXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "3f1f9a1e-c746-4fc2-bd64-727c4180717b"
      },
      "source": [
        "txt = \"Glass -0.41236 0.35532 -0.076236 0.43268 0.14769 -1.1381 -0.30427 0.37039 -0.3804 0.029443 -0.081454 0.15791 -0.32558 -0.35445 0.11918 -0.13342 -0.087387 1.2125 -0.048223\"\n",
        "embeddings_index = dict(get_coefs(*txt.split(\" \"))\n",
        "#print(type(embeddings_index))\n",
        "print(embeddings_index)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-b0d0c10c7c55>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print(embeddings_index)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtGLP057WpuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "90240b2f-b5a2-448b-e24b-357dae3c278b"
      },
      "source": [
        "def get_coefs(word,*arr):\n",
        "  print(word)\n",
        "  print(np.asarray(arr, dtype='float32'))\n",
        "  return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(\"myfile1.txt\"))\n",
        "print(type(embeddings_index))\n",
        "print(embeddings_index)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glass\n",
            "[-0.41236   0.35532  -0.076236  0.43268   0.14769  -1.1381   -0.30427\n",
            "  0.37039  -0.3804    0.029443 -0.081454  0.15791  -0.32558  -0.35445\n",
            "  0.11918  -0.13342  -0.087387  1.2125   -0.048223]\n",
            "Water\n",
            "[-0.41236   0.35532  -0.076236  0.43268   0.14769  -1.1381   -0.30427\n",
            "  0.37039  -0.3804    0.029443 -0.081454  0.15791  -0.32558  -0.35445\n",
            "  0.11918  -0.13342  -0.087387  1.2125   -0.048223]\n",
            "<class 'dict'>\n",
            "{'Glass': array([-0.41236 ,  0.35532 , -0.076236,  0.43268 ,  0.14769 , -1.1381  ,\n",
            "       -0.30427 ,  0.37039 , -0.3804  ,  0.029443, -0.081454,  0.15791 ,\n",
            "       -0.32558 , -0.35445 ,  0.11918 , -0.13342 , -0.087387,  1.2125  ,\n",
            "       -0.048223], dtype=float32), 'Water': array([-0.41236 ,  0.35532 , -0.076236,  0.43268 ,  0.14769 , -1.1381  ,\n",
            "       -0.30427 ,  0.37039 , -0.3804  ,  0.029443, -0.081454,  0.15791 ,\n",
            "       -0.32558 , -0.35445 ,  0.11918 , -0.13342 , -0.087387,  1.2125  ,\n",
            "       -0.048223], dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcTcwuPWbfDN",
        "colab_type": "text"
      },
      "source": [
        "**Load embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGCu64jZbbzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove(word_index):\n",
        "    # EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
        "    EMBEDDING_FILE = 'glove.840B.300d/glove.840B.300d.txt'\n",
        "    ## This file contains content as following\n",
        "    #Glass -0.41236 0.35532 -0.076236 0.43268 0.14769 -1.1381 -0.30427 0.37039 -0.3804 0.029443 -0.081454 0.15791 -0.32558 -0.35445 0.11918 -0.13342 -0.087387 1.2125 -0.048223  .....\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = -0.005838499,0.48782197\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    # word_index = tokenizer.word_index\n",
        "    nb_words = min(max_features, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix \n",
        "    \n",
        "def load_fasttext(word_index):    \n",
        "    # EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
        "    EMBEDDING_FILE = 'wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    # word_index = tokenizer.word_index\n",
        "    nb_words = min(max_features, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def load_para(word_index):\n",
        "    # EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
        "    EMBEDDING_FILE = 'paragram_300_sl999/paragram_300_sl999.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
        "    embed_size = all_embs.shape[1]\n",
        "    print(emb_mean,emb_std,\"para\")\n",
        "\n",
        "    # word_index = tokenizer.word_index\n",
        "    nb_words = min(max_features, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2tNDyGWbniW",
        "colab_type": "text"
      },
      "source": [
        "**CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-NdI6ZebpB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/yekenot/2dcnn-textclassifier\n",
        "def model_cnn(embedding_matrix):\n",
        "    filter_sizes = [1,2,3,5]\n",
        "    num_filters = 36\n",
        "\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    x = Reshape((maxlen, embed_size, 1))(x)\n",
        "\n",
        "    maxpool_pool = []\n",
        "    for i in range(len(filter_sizes)):\n",
        "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n",
        "                                     kernel_initializer='he_normal', activation='elu')(x)\n",
        "        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
        "\n",
        "    z = Concatenate(axis=1)(maxpool_pool)   \n",
        "    z = Flatten()(z)\n",
        "    z = Dropout(0.1)(z)\n",
        "\n",
        "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tga83GlUbs_Y",
        "colab_type": "text"
      },
      "source": [
        "**Attention layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr-AdLAWbtwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMQIkrBYbxG0",
        "colab_type": "text"
      },
      "source": [
        "**LSTM models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNw_GQSGbx2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_lstm_atten(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    #x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
        "    #x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "    x = Attention(maxlen)(x)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ1BUnAGb0Du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_gru_srk_atten(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    #x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
        "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
        "    x = Attention(maxlen)(x) # New\n",
        "    x = Dense(16, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rhHM65Nb2AV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_lstm_du(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    #x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
        "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(64, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm_hpqWlb4Uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_gru_atten_3(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    #x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
        "    #x = Bidirectional(CuDNNGRU(100, return_sequences=True))(x)\n",
        "    #x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
        "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
        "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
        "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
        "    x = Attention(maxlen)(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvztRYqwb7rz",
        "colab_type": "text"
      },
      "source": [
        "**Train and predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1H0ohzhb-Cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
        "def train_pred(model, epochs=2):\n",
        "    for e in range(epochs):\n",
        "        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))\n",
        "        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
        "    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
        "    return pred_val_y, pred_test_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roWxUYA3cGj7",
        "colab_type": "text"
      },
      "source": [
        "**Main part: load, train, pred and blend**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvWX46DscHSC",
        "colab_type": "code",
        "outputId": "35a3f9ba-0c6c-450a-8ee7-0fd274548548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "train_X, val_X, test_X, train_y, val_y, word_index = load_and_prec()\n",
        "vocab = []\n",
        "for w,k in word_index.items():\n",
        "    vocab.append(w)\n",
        "    if k >= max_features:\n",
        "        break\n",
        "embedding_matrix_1 = load_glove(word_index)\n",
        "# embedding_matrix_2 = load_fasttext(word_index)\n",
        "embedding_matrix_3 = load_para(word_index)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (1306122, 3)\n",
            "Test shape :  (375806, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-0.0053247833 0.49346462 para\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KuchLQvgDVI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8be49fe5-d9a2-4b0e-906e-f8dd1dfea07e"
      },
      "source": [
        "model_cnn(embedding_matrix_1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 70)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 70, 300)      28500000    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 70, 300, 1)   0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 70, 1, 36)    10836       reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 69, 1, 36)    21636       reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 68, 1, 36)    32436       reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 66, 1, 36)    54036       reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4, 1, 36)     0           max_pooling2d_1[0][0]            \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "                                                                 max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 144)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 144)          0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            145         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 28,619,089\n",
            "Trainable params: 28,619,089\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7fd2f3496128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8etObAdscKmg",
        "colab_type": "code",
        "outputId": "8df63750-9b00-4d96-dafd-afd92b35199c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Simple average: http://aclweb.org/anthology/N18-2031\n",
        "\n",
        "# We have presented an argument for averaging as\n",
        "# a valid meta-embedding technique, and found experimental\n",
        "# performance to be close to, or in some cases \n",
        "# better than that of concatenation, with the\n",
        "# additional benefit of reduced dimensionality  \n",
        "\n",
        "\n",
        "## Unweighted DME in https://arxiv.org/pdf/1804.07983.pdf\n",
        "\n",
        "# â€œThe downside of concatenating embeddings and \n",
        "#  giving that as input to an RNN encoder, however,\n",
        "#  is that the network then quickly becomes inefficient\n",
        "#  as we combine more and more embeddings.â€\n",
        "  \n",
        "# embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2, embedding_matrix_3], axis = 0)\n",
        "embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_3], axis = 0)\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lE3bJpHcLcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcdZd4yfcEFy",
        "colab_type": "code",
        "outputId": "3bec1cc8-363d-4f63-aa0c-4add60b54441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "pred_val_y, pred_test_y = train_pred(model_gru_atten_3(embedding_matrix), epochs = 3)\n",
        "outputs.append([pred_val_y, pred_test_y, '3 GRU w/ atten'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNN' used by {{node bidirectional_1/CudnnRNN}}with these attrs: [dropout=0, seed=87654321, input_mode=\"linear_input\", T=DT_FLOAT, direction=\"unidirectional\", rnn_mode=\"gru\", is_training=true, seed2=0]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[bidirectional_1/CudnnRNN]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-b748d9f69915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_val_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_gru_atten_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_val_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'3 GRU w/ atten'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-b4c7a87f47e0>\u001b[0m in \u001b[0;36mmodel_gru_atten_3\u001b[0;34m(embedding_matrix)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_gru_atten_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#x = Bidirectional(CuDNNGRU(100, return_sequences=True))(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2680\u001b[0m     \"\"\"\n\u001b[1;32m   2681\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 216\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNN' used by node bidirectional_1/CudnnRNN (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [dropout=0, seed=87654321, input_mode=\"linear_input\", T=DT_FLOAT, direction=\"unidirectional\", rnn_mode=\"gru\", is_training=true, seed2=0]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[bidirectional_1/CudnnRNN]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmItoGzwcPEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_val_y, pred_test_y = train_pred(model_gru_srk_atten(embedding_matrix), epochs = 2)\n",
        "outputs.append([pred_val_y, pred_test_y, 'gru atten srk'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzdmyzB0cQ2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_val_y, pred_test_y = train_pred(model_cnn(embedding_matrix), epochs = 2)\n",
        "outputs.append([pred_val_y, pred_test_y, '2d CNN'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUII5_8GcSx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_val_y, pred_test_y = train_pred(model_cnn(embedding_matrix_1), epochs = 2) # GloVe only\n",
        "outputs.append([pred_val_y, pred_test_y, '2d CNN GloVe'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOjUG5mYcUcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_val_y, pred_test_y = train_pred(model_lstm_du(embedding_matrix), epochs = 2)\n",
        "outputs.append([pred_val_y, pred_test_y, 'LSTM DU'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFnumPhMcWDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_val_y, pred_test_y = train_pred(model_lstm_atten(embedding_matrix), epochs = 3)\n",
        "outputs.append([pred_val_y, pred_test_y, '2 LSTM w/ attention'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9IMl7BxcYUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_val_y, pred_test_y = train_pred(model_lstm_atten(embedding_matrix_1), epochs = 3) # Only GloVe\n",
        "outputs.append([pred_val_y, pred_test_y, '2 LSTM w/ attention GloVe'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUhZvYTFcaDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_val_y, pred_test_y = train_pred(model_lstm_atten(embedding_matrix_3), epochs = 3) # Only Para\n",
        "outputs.append([pred_val_y, pred_test_y, '2 LSTM w/ attention Para'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCJvkSccdwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pred_test_y = np.sum([outputs[i][1] * weights[i] for i in range(len(outputs))], axis = 0)\n",
        "pred_test_y = np.mean([outputs[i][1] for i in range(len(outputs))], axis = 0)\n",
        "\n",
        "pred_test_y = (pred_test_y > 0.33).astype(int)\n",
        "test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\n",
        "out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
        "out_df['prediction'] = pred_test_y\n",
        "out_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI-Qr_J3b6C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEVX25UHbkxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## https://www.kaggle.com/suicaokhoailang/beating-the-baseline-with-one-weird-trick-0-691\n",
        "## Khoi Nguyen"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}